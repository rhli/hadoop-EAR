#!/usr/bin/env bash

# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# The Hadoop command script
#
# Environment Variables
#
#   JAVA_HOME        The java implementation to use.  Overrides JAVA_HOME.
#
#   HADOOP_CLASSPATH Extra Java CLASSPATH entries.
#
#   HADOOP_HEAPSIZE  The maximum amount of heap to use, in MB.
#                    Default is 1000.
#
#   HADOOP_OPTS      Extra Java runtime options.
#
#   HADOOP_NAMENODE_OPTS       These options are added to HADOOP_OPTS
#   HADOOP_CLIENT_OPTS         when the respective command is run.
#   HADOOP_{COMMAND}_OPTS etc  HADOOP_JT_OPTS applies to JobTracker
#                              for e.g.  HADOOP_CLIENT_OPTS applies to
#                              more than one command (fs, dfs, fsck,
#                              dfsadmin etc)
#
#   HADOOP_CONF_DIR  Alternate conf dir. Default is ${HADOOP_HOME}/conf.
#
#   HADOOP_ROOT_LOGGER The root appender. Default is INFO,console
#

thisfile="${BASH_SOURCE[0]}"
if [ -L "${thisfile}" ]; then
  thisfile=$(readlink "${thisfile}")
fi

bin=`dirname "${thisfile}"`
bin=`cd "$bin"; pwd`

. "$bin"/hadoop-config.sh

cygwin=false
case "`uname`" in
CYGWIN*) cygwin=true;;
esac

# if no args specified, show usage
if [ $# = 0 ]; then
  echo "Usage: hadoop [--config confdir] COMMAND"
  echo "where COMMAND is one of:"
  echo "  namenode -format     format the DFS filesystem"
  echo "  secondarynamenode    run the DFS secondary namenode"
  echo "  namenode             run the DFS namenode"
  echo "  datanode             run a DFS datanode"
  echo "  dfsadmin             run a DFS admin client"
  echo "  mradmin              run a Map-Reduce admin client"
  echo "  coronaadmin          run a Corona admin client"
  echo "  fsck                 run a DFS filesystem checking utility"
  echo "  avatarfsck           run a avatar DFS filesystem checking utility"
  echo "  raidfsck [path]      run RAID-aware filesystem checking utility"
  echo "  raidshell [options]  run RAID-shell utility"
  echo "  fs                   run a generic filesystem user client"
  echo "  balancer             run a cluster balancing utility"
  echo "  avatarbalancer       run a avatar cluster balancing utility"
  echo "  jmxget               get JMX exported values from NameNode or DataNode."
  echo "  oiv                  apply the offline fsimage viewer to an fsimage"
  echo "  oev                  apply the offline edits viewer to an edits file"
  echo "  oid                  apply the offline fsimage decompressor to an fsimage"
  echo "                       Use -help to see options"
  echo "  jobtracker           run the MapReduce job Tracker node"
  echo "  pipes                run a Pipes job"
  echo "  tasktracker          run a MapReduce task Tracker node"
  echo "  job                  manipulate MapReduce jobs"
  echo "  queue                get information regarding JobQueues"
  echo "  version              print the version"
  echo "  jar <jar>            run a jar file"
  echo "  onejar <jar>         run a jar file packaged using one-jar, do not specify name of main class"
  echo "  distcp <srcurl> <desturl> copy file or directories recursively"
  echo "  fastcopy <src file> <dest file> copy files by maintaining optimal locality"
  echo "  archive -archiveName NAME -p <parent path> <src>* <dest> create a hadoop archive"
  echo "  daemonlog            get/set the log level for each daemon"
  echo " or"
  echo "  CLASSNAME            run the class named CLASSNAME"
  echo "  getconf              get config values from configuration"
  echo "Most commands print help when invoked w/o parameters."
  exit 1
fi

# get arguments
COMMAND=$1
shift

if [ -f "${HADOOP_CONF_DIR}/hadoop-env.sh" ]; then
  . "${HADOOP_CONF_DIR}/hadoop-env.sh"
fi

# some Java parameters
if [ "$JAVA_HOME" != "" ]; then
  #echo "run java in $JAVA_HOME"
  JAVA_HOME=$JAVA_HOME
fi

if [ "$JAVA_HOME" = "" ]; then
  echo "Error: JAVA_HOME is not set."
  exit 1
fi

JAVA=$JAVA_HOME/bin/java
JAVA_HEAP_MAX=-Xmx1000m

# check envvars which might override default args
if [ "$HADOOP_HEAPSIZE" != "" ]; then
  #echo "run with heapsize $HADOOP_HEAPSIZE"
  JAVA_HEAP_MAX="-Xmx""$HADOOP_HEAPSIZE""m"
  #echo $JAVA_HEAP_MAX
fi

# CLASSPATH initially contains $HADOOP_CONF_DIR
JMX_OPTS=""
CLASSPATH="${HADOOP_CONF_DIR}"
CLASSPATH=${CLASSPATH}:$HADOOP_CLASSPATH
CLASSPATH=${CLASSPATH}:$JAVA_HOME/lib/tools.jar

# for developers, add Hadoop classes to CLASSPATH
if [ -d "$HADOOP_HOME/build/classes" ]; then
  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build/classes
fi
if [ -d "$HADOOP_HOME/build/webapps" ]; then
  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build
fi
if [ -d "$HADOOP_HOME/build/test/classes" ]; then
  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build/test/classes
fi
if [ -d "$HADOOP_HOME/build/tools" ]; then
  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build/tools
fi
if [ -d "$HADOOP_HOME/build/contrib/highavailability/classes" ]; then
  CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build/contrib/highavailability/classes
fi

# so that filenames w/ spaces are handled correctly in loops below
IFS=

# for releases, add core hadoop jar & webapps to CLASSPATH
if [ -d "$HADOOP_HOME/webapps" ]; then
  CLASSPATH=${CLASSPATH}:$HADOOP_HOME
fi
for f in $HADOOP_HOME/hadoop-*-core.jar; do
  CLASSPATH=${CLASSPATH}:$f;
done

# add libs to CLASSPATH
# the thrift library has conflicts in hdfs-nfs-proxy
# only put neccessary jars for hdfs-nfs-proxy
if [ "$COMMAND" = "hdfsnfsproxy" ] ; then
  for f in $HADOOP_HOME/lib/slf4j*.jar; do
    CLASSPATH=${CLASSPATH}:$f;
  done
  for f in $HADOOP_HOME/lib/zookeeper*.jar; do
    CLASSPATH=${CLASSPATH}:$f;
  done
  for f in $HADOOP_HOME/lib/guava*.jar; do
    CLASSPATH=${CLASSPATH}:$f;
  done
  for f in $HADOOP_HOME/lib/json*.jar; do
    CLASSPATH=${CLASSPATH}:$f;
  done
  for f in $HADOOP_HOME/lib/commons*.jar; do
    CLASSPATH=${CLASSPATH}:$f;
  done
  for f in $HADOOP_HOME/lib/log4j*.jar; do
    CLASSPATH=${CLASSPATH}:$f;
  done
  for f in $HADOOP_HOME/lib/hadoop*.jar; do
    CLASSPATH=${CLASSPATH}:$f;
  done
else
  for f in $HADOOP_HOME/lib/*.jar; do
    CLASSPATH=${CLASSPATH}:$f;
  done
fi

if [ -d "$HADOOP_HOME/build/ivy/lib/Hadoop/common" ]; then
  if [ "$COMMAND" = "hdfsnfsproxy" ] ; then
    for f in $HADOOP_HOME/build/ivy/lib/Hadoop/common/slf4j*.jar; do
      CLASSPATH=${CLASSPATH}:$f;
    done
    for f in $HADOOP_HOME/build/ivy/lib/Hadoop/common/zookeeper*.jar; do
      CLASSPATH=${CLASSPATH}:$f;
    done
    for f in $HADOOP_HOME/build/ivy/lib/Hadoop/common/guava*.jar; do
      CLASSPATH=${CLASSPATH}:$f;
    done
    for f in $HADOOP_HOME/build/ivy/lib/Hadoop/common/json*.jar; do
      CLASSPATH=${CLASSPATH}:$f;
    done
    for f in $HADOOP_HOME/build/ivy/lib/Hadoop/common/commons*.jar; do
      CLASSPATH=${CLASSPATH}:$f;
    done
    for f in $HADOOP_HOME/build/ivy/lib/Hadoop/common/log4j*.jar; do
      CLASSPATH=${CLASSPATH}:$f;
    done
    for f in $HADOOP_HOME/build/ivy/lib/Hadoop/common/hadoop*.jar; do
      CLASSPATH=${CLASSPATH}:$f;
    done
  else
   for f in $HADOOP_HOME/build/ivy/lib/Hadoop/common/*.jar; do
      CLASSPATH=${CLASSPATH}:$f;
   done
  fi
fi

for f in $HADOOP_HOME/lib/jsp-2.1/*.jar; do
  CLASSPATH=${CLASSPATH}:$f;
done

for f in $HADOOP_HOME/hadoop-*-tools.jar; do
  TOOL_PATH=${TOOL_PATH}:$f;
done
for f in $HADOOP_HOME/build/hadoop-*-tools.jar; do
  TOOL_PATH=${TOOL_PATH}:$f;
done

# CORONA_PATH for corona daemons
if [ -d "$HADOOP_HOME/build/contrib/corona/classes" ]; then
  CORONA_PATH=${CORONA_PATH}:$HADOOP_HOME/build/contrib/corona/classes
fi

for f in $HADOOP_HOME/contrib/corona/*.jar; do
  CORONA_PATH=${CORONA_PATH}:$f;
done

if [ "$COMMAND" != "hdfsnfsproxy" ] ; then
  if [ "$CORONA_PATH" != "" ]; then
    CLASSPATH=${CLASSPATH}:${CORONA_PATH}
  fi
fi

for f in $HADOOP_HOME/contrib/corona/lib/*.jar; do
  CORONA_LIB_PATH=${CORONA_LIB_PATH}:$f;
done

# NOTIFIER_PATH for the namespace notifier server daemon
if [ -d "$HADOOP_HOME/build/contrib/namespace-notifier/classes" ]; then
  NOTIFIER_PATH=${NOTIFIER_PATH}:$HADOOP_HOME/build/contrib/namespace-notifier/classes
fi

for f in $HADOOP_HOME/contrib/namespace-notifier/*.jar; do
  NOTIFIER_PATH=${NOTIFIER_PATH}:$f;
done

# NFS_PROXY_PATH for the hdfs nfs proxy daemon
if [ -d "$HADOOP_HOME/build/contrib/hdfs-nfs-proxy/classes" ] ; then
  NFS_PROXY_PATH=${NFS_PROXY_PATH}:$HADOOP_HOME/build/contrib/hdfs-nfs-proxy/classes
fi

# hdfs-nfs-proxy needs raid and configerator in classpath
# to get DistributedRaidFileSystem from configuration
if [ -d "$HADOOP_HOME/build/contrib/raid/classes" ] ; then
    NFS_PROXY_PATH=${NFS_PROXY_PATH}:$HADOOP_HOME/build/contrib/raid/classes
fi

if [ -d "$HADOOP_HOME/build/contrib/configerator/classes" ] ; then
    NFS_PROXY_PATH=${NFS_PROXY_PATH}:$HADOOP_HOME/build/contrib/configerator/classes
fi

for f in $HADOOP_HOME/build/contrib/hdfs-nfs-proxy/lib/*.jar; do
  NFS_PROXY_PATH=${NFS_PROXY_PATH}:$f;
done

for f in $HADOOP_HOME/build/contrib/hdfs-nfs-proxy/hadoop-*-hdfs-nfs-proxy.jar; do
  NFS_PROXY_PATH=${NFS_PROXY_PATH}:$f;
done

for f in $HADOOP_HOME/contrib/hdfs-nfs-proxy/*.jar; do
  NFS_PROXY_PATH=${NFS_PROXY_PATH}:$f;
done

for f in $HADOOP_HOME/contrib/hdfs-nfs-proxy/lib/*.jar; do
  NFS_PROXY_PATH=${NFS_PROXY_PATH}:$f;
done

# default log directory & file
if [ "$HADOOP_LOG_DIR" = "" ]; then
  HADOOP_LOG_DIR="$HADOOP_HOME/logs"
fi
if [ "$HADOOP_LOGFILE" = "" ]; then
  HADOOP_LOGFILE='hadoop.log'
fi

# default policy file for service-level authorization
if [ "$HADOOP_POLICYFILE" = "" ]; then
  HADOOP_POLICYFILE="hadoop-policy.xml"
fi

if [ "$HADOOP_GC_LOG_OPTS" != "" ]; then
  HADOOP_GC_LOG_OPTS="${HADOOP_GC_LOG_OPTS}$HADOOP_LOG_DIR/hadoop-$HADOOP_IDENT_STRING-$COMMAND-gc.log"
fi

# restore ordinary behaviour
unset IFS

# Enable assertions for mapred for corona testing
HADOOP_OPTS="$HADOOP_OPTS -ea:org.apache.hadoop.mapred..."

# figure out which class to run
if [ "$COMMAND" = "namenode" ] ; then
  CLASS='org.apache.hadoop.hdfs.server.namenode.NameNode'
  JMX_OPTS=$HADOOP_JMX_NAMENODE_OPTS
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_GC_LOG_OPTS $HADOOP_NAMENODE_OPTS"
elif [ "$COMMAND" = "avatarshell" ] ; then
  CLASS='org.apache.hadoop.hdfs.AvatarShell'
  HADOOP_LOGFILE='avatarshell.log'
  HADOOP_ROOT_LOGGER=INFO,DRFA
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_GC_LOG_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "avatarzk" ] ; then
  CLASS='org.apache.hadoop.hdfs.AvatarZKShell'
  HADOOP_LOGFILE='avatarzkshell.log'
  HADOOP_ROOT_LOGGER=INFO,DRFA
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_GC_LOG_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "avatarnode" ] ; then
  CLASS='org.apache.hadoop.hdfs.server.namenode.AvatarNode'
  JMX_OPTS=$HADOOP_JMX_NAMENODE_OPTS
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_GC_LOG_OPTS $HADOOP_NAMENODE_OPTS"
elif [ "$COMMAND" = "journalnode" ] ; then
  CLASS='org.apache.hadoop.hdfs.qjournal.server.JournalNode'
  JMX_OPTS=$HADOOP_JMX_JOURNAL_OPTS
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_GC_LOG_OPTS $HADOOP_JOURNAL_OPTS"
elif [ "$COMMAND" = "secondarynamenode" ] ; then
  CLASS='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode'
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_GC_LOG_OPTS $HADOOP_SECONDARYNAMENODE_OPTS"
elif [ "$COMMAND" = "raidnode" ] ; then
  CLASS='org.apache.hadoop.raid.RaidNode'
  JMX_OPTS=$HADOOP_JMX_RAIDNODE_OPTS
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_GC_LOG_OPTS"
  CLASSPATH=${CORONA_LIB_PATH}:${CLASSPATH}
elif [ "$COMMAND" = "notifier" ] ; then
  CLASS='org.apache.hadoop.hdfs.notifier.server.ServerCore'
  if [ "$NOTIFIER_PATH" != "" ]; then
    CLASSPATH=${CLASSPATH}:${NOTIFIER_PATH}
  fi
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_GC_LOG_OPTS $NOTIFIER_OPTS"
  JMX_OPTS="$JMX_OPTS $NOTIFIER_JMX_OPTS"
elif [ "$COMMAND" = "hdfsnfsproxy" ] ; then
  CLASS='org.apache.hadoop.hdfs.nfs.nfs4.NFS4Server'
  # hdfs-nfs-proxy is using log4j Rolling File Appender for
  # log rotation, max log size, and how many backup logs to keep
  HADOOP_LOGFILE='hdfs-nfs-proxy.log'
  HADOOP_ROOT_LOGGER=INFO,RFA
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_GC_LOG_OPTS"
  JMX_OPTS="$JMX_OPTS"
  if [ "$NFS_PROXY_PATH" != "" ]; then
    CLASSPATH=${CLASSPATH}:${NFS_PROXY_PATH}
  fi
elif [ "$COMMAND" = "fsshellservice" ] ; then
  CLASS='org.apache.hadoop.hdfs.fsshellservice.FsShellServiceImpl'
  if [ -d "$HADOOP_HOME/build/contrib/corona/lib" ]; then
    for f in $HADOOP_HOME/build/contrib/corona/lib/*.jar; do
      CLASSPATH=${CLASSPATH}:$f;
    done
  fi
  if [ -d "$HADOOP_HOME/build/contrib/fsshellservice/" ]; then
    CLASSPATH=${CLASSPATH}:$HADOOP_HOME/build/contrib/fsshellservice/classes
  fi
  for f in $HADOOP_HOME/contrib/fsshellservice/*.jar; do
    CLASSPATH=${CLASSPATH}:$f;
  done
  CLASSPATH=${CORONA_LIB_PATH}:${CLASSPATH}
elif [ "$COMMAND" = "avatardatanode" ] ; then
  CLASS='org.apache.hadoop.hdfs.server.datanode.AvatarDataNode'
  JMX_OPTS=$HADOOP_JMX_DATANODE_OPTS
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_GC_LOG_OPTS $HADOOP_DATANODE_OPTS"
  HADOOP_ROOT_LOGGER=${HADOOP_DATANODE_LOGGER:-$HADOOP_ROOT_LOGGER}
elif [ "$COMMAND" = "datanode" ] ; then
  CLASS='org.apache.hadoop.hdfs.server.datanode.DataNode'
  JMX_OPTS=$HADOOP_JMX_DATANODE_OPTS
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_GC_LOG_OPTS $HADOOP_DATANODE_OPTS"
  HADOOP_ROOT_LOGGER=${HADOOP_DATANODE_LOGGER:-$HADOOP_ROOT_LOGGER}
elif [ "$COMMAND" = "fs" ] ; then
  CLASS=org.apache.hadoop.fs.FsShell
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "dfs" ] ; then
  CLASS=org.apache.hadoop.fs.FsShell
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "dfsadmin" ] ; then
  CLASS=org.apache.hadoop.hdfs.tools.DFSAdmin
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "mradmin" ] ; then
  CLASS=org.apache.hadoop.mapred.tools.MRAdmin
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "version" ] ; then
  CLASS=org.apache.hadoop.util.VersionInfo
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "fsck" ] ; then
  CLASS=org.apache.hadoop.hdfs.tools.DFSck
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "avatarfsck" ] ; then
  CLASS=org.apache.hadoop.hdfs.tools.AvatarDFSck
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "raidfsck" ] ; then
  CLASS=org.apache.hadoop.raid.RaidShell
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
  CMDLINE_OPTS="-fsck $CMDLINE_OPTS"
elif [ "$COMMAND" = "raidshell" ] ; then
  CLASS=org.apache.hadoop.raid.RaidShell
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "notifiershell" ] ; then
  CLASS=org.apache.hadoop.hdfs.notifier.tools.NotifierShell
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "balancer" ] ; then
  CLASS=org.apache.hadoop.hdfs.server.balancer.Balancer
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_BALANCER_OPTS"
  CMDLINE_OPTS="$CMDLINE_OPTS $BALANCER_CMDLINE_OPTS"
elif [ "$COMMAND" = "avatarbalancer" ] ; then
  CLASS=org.apache.hadoop.hdfs.server.balancer.AvatarBalancer
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_BALANCER_OPTS"
  CMDLINE_OPTS="$CMDLINE_OPTS $BALANCER_CMDLINE_OPTS"
elif [ "$COMMAND" = "oiv" ] ; then
  CLASS=org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "oev" ] ; then
  CLASS=org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsViewer
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "oid" ] ; then
  CLASS=org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageDecompressor
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "jmxget" ] ; then
  CLASS=org.apache.hadoop.hdfs.tools.JMXGet
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "jobtracker" ] ; then
  CLASS=org.apache.hadoop.mapred.JobTracker
  JMX_OPTS=$HADOOP_JMX_JOBTRACKER_OPTS
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_GC_LOG_OPTS $HADOOP_JOBTRACKER_OPTS"
  if [ -n "$HADOOP_INSTANCE" ] ; then
    CMDLINE_OPTS="-instance $HADOOP_INSTANCE $CMDLINE_OPTS"
  fi
elif [ "$COMMAND" = "coronaclustermanager" ] ; then
  CLASS=org.apache.hadoop.corona.ClusterManagerServer
  JMX_OPTS=$HADOOP_JMX_CORONACLUSTERMANAGER_OPTS
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_GC_LOG_OPTS $HADOOP_CORONACLUSTERMANAGER_OPTS"
  # Corona lib path should be first to ensure that it uses the right thrift JAR
  CLASSPATH=${CORONA_LIB_PATH}:${CLUSTER_MANAGER_LIB_PATH}:${CLASSPATH}
elif [ "$COMMAND" = "coronatasktracker" ] ; then
  CLASS=org.apache.hadoop.mapred.CoronaTaskTracker
  JMX_OPTS=$HADOOP_JMX_TASKTRACKER_OPTS
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_GC_LOG_OPTS $HADOOP_TASKTRACKER_OPTS"
  # For corona task trackers, the tasks should not get the thrift library.
  MAPREDUCE_TASK_SYSTEM_CLASSPATH=${CLASSPATH}
  export MAPREDUCE_TASK_SYSTEM_CLASSPATH
  # See coronaclustermanager comment
  CLASSPATH=${CORONA_LIB_PATH}:${CLASSPATH}
elif [ "$COMMAND" = "coronaproxyjobtracker" ] ; then
  CLASS=org.apache.hadoop.mapred.ProxyJobTracker
  JMX_OPTS=$HADOOP_JMX_CORONAPROXYJOBTRACKER_OPTS
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_GC_LOG_OPTS $HADOOP_CORONAPROXYJOBTRACKER_OPTS"
  # See coronaclustermanager comment
  CLASSPATH=${CORONA_LIB_PATH}:${CLASSPATH}
elif [ "$COMMAND" = "coronaclient" ] ; then
  CLASS=org.apache.hadoop.corona.CoronaClient
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
  CLASSPATH=${CORONA_LIB_PATH}:${CLASSPATH}
elif [ "$COMMAND" = "coronaadmin" ] ; then
  CLASS=org.apache.hadoop.corona.CoronaAdmin
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
  CLASSPATH=${CORONA_LIB_PATH}:${CLASSPATH}
elif [ "$COMMAND" = "tasktracker" ] ; then
  CLASS=org.apache.hadoop.mapred.TaskTracker
  JMX_OPTS=$HADOOP_JMX_TASKTRACKER_OPTS
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_GC_LOG_OPTS $HADOOP_TASKTRACKER_OPTS"
  HADOOP_ROOT_LOGGER=${HADOOP_TASKTRACKER_LOGGER:-$HADOOP_ROOT_LOGGER}
  if [ -n "$HADOOP_INSTANCE" ] ; then
    CMDLINE_OPTS="-instance $HADOOP_INSTANCE $CMDLINE_OPTS"
  fi
elif [ "$COMMAND" = "multitasktracker" ] ; then
  CLASS=org.apache.hadoop.mapred.MultiTaskTracker
  HADOOP_ROOT_LOGGER=${HADOOP_TASKTRACKER_LOGGER:-$HADOOP_ROOT_LOGGER}
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_MULTITASKTRACKER_OPTS"
  # This should be the number of tasktrackers
  if [ -n "$MULTI_TT_OPTIONS" ] ; then
    CMDLINE_OPTS="$MULTI_TT_OPTIONS"
  else
    CMDLINE_OPTS="1"
  fi
elif [ "$COMMAND" = "job" ] ; then
  CLASS=org.apache.hadoop.mapred.JobClient
elif [ "$COMMAND" = "queue" ] ; then
  CLASS=org.apache.hadoop.mapred.JobQueueClient
elif [ "$COMMAND" = "pipes" ] ; then
  CLASS=org.apache.hadoop.mapred.pipes.Submitter
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "version" ] ; then
  CLASS=org.apache.hadoop.util.VersionInfo
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "onejar" ] ; then
  CLASS=org.apache.hadoop.util.RunJar
  HADOOP_OPTS="$HADOOP_OPTS -Done-jar.jar.path=$1"
elif [ "$COMMAND" = "jar" ] ; then
  CLASS=org.apache.hadoop.util.RunJar
elif [ "$COMMAND" = "fastcopy" ] ; then
  CLASS=org.apache.hadoop.hdfs.tools.FastCopy
  CLASSPATH=${CLASSPATH}:${TOOL_PATH}
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "distcp" ] ; then
  CLASS=org.apache.hadoop.tools.DistCp
  CLASSPATH=${CORONA_LIB_PATH}:${CLASSPATH}:${TOOL_PATH}
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "daemonlog" ] ; then
  CLASS=org.apache.hadoop.log.LogLevel
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "archive" ] ; then
  CLASS=org.apache.hadoop.tools.HadoopArchives
  CLASSPATH=${CLASSPATH}:${TOOL_PATH}
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "sampler" ] ; then
  CLASS=org.apache.hadoop.mapred.lib.InputSampler
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "hourglass" ] ; then
  CLASS=org.apache.hadoop.mapred.HourGlass
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "fairscheduler" ] ; then
  CLASS=org.apache.hadoop.mapred.FairSchedulerShell
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
elif [ "$COMMAND" = "getconf" ] ; then
  CLASS=org.apache.hadoop.tools.GetConf
  HADOOP_OPTS="$HADOOP_OPTS $HADOOP_CLIENT_OPTS"
else
  CLASS=$COMMAND
fi

# cygwin path translation
if $cygwin; then
  CLASSPATH=`cygpath -p -w "$CLASSPATH"`
  HADOOP_HOME=`cygpath -w "$HADOOP_HOME"`
  HADOOP_LOG_DIR=`cygpath -w "$HADOOP_LOG_DIR"`
  TOOL_PATH=`cygpath -p -w "$TOOL_PATH"`
fi
# setup 'java.library.path' for native-hadoop code if necessary
JAVA_LIBRARY_PATH=''
if [ -d "${HADOOP_HOME}/build/native" -o -d "${HADOOP_HOME}/lib/native" ]; then

  if [ "x$JAVA_PLATFORM" == "x" ]; then
    JAVA_PLATFORM=`CLASSPATH=${CLASSPATH} ${JAVA} -Xmx32m ${HADOOP_JAVA_PLATFORM_OPTS} org.apache.hadoop.util.PlatformName | sed -e "s/ /_/g"`
  fi

  if [ -d "$HADOOP_HOME/build/native" ]; then
    JAVA_LIBRARY_PATH=${HADOOP_HOME}/build/native/${JAVA_PLATFORM}/lib
  fi

  if [ -d "${HADOOP_HOME}/lib/native" ]; then
    if [ "x$JAVA_LIBRARY_PATH" != "x" ]; then
      JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}:${HADOOP_HOME}/lib/native/${JAVA_PLATFORM}
    else
      JAVA_LIBRARY_PATH=${HADOOP_HOME}/lib/native/${JAVA_PLATFORM}
    fi
  fi
fi

# use fbcode gcc in nfs proxy java.library.path
if [ "$COMMAND" = "hdfsnfsproxy" ] ; then
  JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}:/usr/local/fbcode/gcc-4.6.2-glibc-2.5.1/lib;
fi

# cygwin path translation
if $cygwin; then
  JAVA_LIBRARY_PATH=`cygpath -p "$JAVA_LIBRARY_PATH"`
fi
export LD_LIBRARY_PATH="$JAVA_LIBRARY_PATH"

HADOOP_OPTS="$HADOOP_OPTS $HADOOP_DAEMON_OPTS"
HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.log.dir=$HADOOP_LOG_DIR"
HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.log.file=$HADOOP_LOGFILE"
HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.home.dir=$HADOOP_HOME"
HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.id.str=$HADOOP_IDENT_STRING"
HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.root.logger=${HADOOP_ROOT_LOGGER:-INFO,console}"
HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.application=${HADOOP_APPLICATION:-default}"
HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.installationid=${CLUSTER_NAME:-default}"
if [ "x$JAVA_LIBRARY_PATH" != "x" ]; then
  HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$JAVA_LIBRARY_PATH"
fi
HADOOP_OPTS="$HADOOP_OPTS -Dhadoop.policy.file=$HADOOP_POLICYFILE"


if [ "$HADOOP_DEPLOYMENT" == "server" ]; then
  # Look for another instance of the class running
  ps -ef | grep $COMMAND | grep 'bin/hadoop'
  INSTANCES=`ps -ef | grep $COMMAND | grep 'bin/hadoop' | wc -l`
  if [ $INSTANCES -gt 2 ]; then
    echo "Another instance of this command is running"
    exit 1
  fi
  JINSTANCES=`ps -ef | grep $CLASS | grep java | wc -l`
  if [ $JINSTANCES -gt 2 ]; then
    echo "Another instance of this command is running"
    exit 1
  fi
fi

# run it
export CLASSPATH
export JVM_PID=$$
exec "$JAVA" $JAVA_HEAP_MAX $HADOOP_OPTS $JMX_OPTS "-Dfb_hadoop_version=0.20" $CLASS $CMDLINE_OPTS "$@"
